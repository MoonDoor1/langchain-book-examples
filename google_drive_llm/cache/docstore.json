{"docstore/data": {"c5cd10c2-a0bc-4cbe-962c-35abc7913f99": {"__data__": {"id_": "c5cd10c2-a0bc-4cbe-962c-35abc7913f99", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "hash": "3b1f0b68e3e782105a04f9626a93f596202212440a1ca99b4774b64107795526", "text": "\ufeffTo test different versions of your pipeline and determine the best approach, you can follow these steps:\n\n\n1. Define Variations: Identify the different variations of your pipeline that you want to test. This could include different vector database structures, different types of questions, different methods of extracting information from the documents, etc.\n\n\n2. Create Test Scenarios: For each variation, define a set of test scenarios. These scenarios should be representative of the real-world situations that your LLM system will encounter. For example, you might create test scenarios based on different types of quarterly earnings reports, different types of queries, etc.\n\n\n3. Run Tests: For each variation and each test scenario, run your pipeline and record the results. This could include the accuracy of the responses, the speed of the pipeline, the size of the database, etc.\n\n\n4. Evaluate Results: Compare the results of the different variations to determine which one performs best. This could involve statistical analysis, visualizations, or other methods of comparing the results.\n\n\n5. Iterate: Based on the results of your tests, make adjustments to your pipeline and repeat the testing process. This iterative process can help you continuously improve your LLM system.\n\n\n6 Automated Testing: Consider setting up automated testing for your LLM system. This could involve writing a script that runs your tests automatically at regular intervals, or whenever changes are made to the system. Automated testing can help catch issues early, before they affect the end users of your system.\n\n\nIn terms of whether to use questions when extracting info from the documents, this depends on the specifics of your LLM system and the nature of the documents and queries. Using questions can help guide the extraction process and ensure that the information extracted is relevant to the queries. However, it may also limit the range of information that is extracted. Embedding and chunking the documents without using questions can capture a wider range of information, but it may also include irrelevant information and make the extraction process less efficient.\n\n\nTo determine the best approach, you could run tests with and without questions and compare the results. This could help you understand the trade-offs between these two approaches and choose the one that best meets your needs.\n\n\n\n\nThe different types of variations I want to test\n\n1. No question loop. This will just take the documents embed them then run testing baseline\n2. My questions, This will run the question loop. Take the answers embed them run testing\n3. Computer generated questions. Generate computer questions based on llama question generations. Embed answers run baseline tests\n4. Few shot learning templates vs Zero Shot learning templates\n5. TBD, Other models, Other functions. Let's wait for tomorrow's call with Allie then re-look at the testing scenarios. \n6. Testing what ever the fuck this is Recursive retrever + agents\n7. This is interesting. Link llama index + search engine for LLMs", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "4"}, "5258a827-5bbe-4090-9024-531ca899be59": {"__data__": {"id_": "5258a827-5bbe-4090-9024-531ca899be59", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "hash": "12b28aa787b31e974d6a9cd38926e7a810a3b33dd4f26bf60477d4078fd17c07", "text": "\ufeff\ud83c\udfae Updated LLM Adventure Saga: Quest for the Ultimate Investment Tool with Timelines \ud83c\udfae\n\ud83c\udf0d World 1: The Realm of Data Gathering\n\u23f3 Timeframe: 1 Day\n* Level 1: \"The InfoScout\" - https://13f.info/ - site that formats 13-f filings open source\n   * Goal: Collect data from emails from Allie or find sample data for this task .\n* Level 2: \"SEC Explorer\"\n   * Goal: Download 10 sample 13-f filings from the SEC website.\n* Level 3: \"The Data Vault\"\n   * Goal: Set up a basic SQL database (you can use Docker for this).\n   * Install software \n   * Init db \n   * Design db \n   * Test db \n   * Documentation \n   * Backup \n\n\n        I'm not going to do the database for now on because of this reason: \n\n\u201chow can you reduce complexity? The less unknowns are in your stack, the easier it\u2019ll be for you to maintain things and react to incidents.\nDatabases are critical services. They take effort to operate, and even more effort to do so reliably. If you really need your data to stick around and be safe no matter what, you don\u2019t want unnecessary risks.\u201d\n\n\n\n\n        We are skipping the database section so now we just need to init the github env\n\nCompleted when:\n* Have 13-f filings - DONE\n* Have Quarterly memo pdfs - DONE\n* Have Git hub env set up - DONE\n\n        \n\ud83c\udf0d World 2: The Preprocessing Kingdom\n\u23f3 Timeframe: 3 Days\n* Level 0: \u201cPine line definition\u201d\n   * Define what you want your pipeline to do and look like. Whether this is a chatbot or a summarizer or a QA over a corpus of documents. \n\n\n* Level 1: \"Text Normalizer\"\n   * Goal: Write a script to normalize your sample text data\n   * Identify Missing Values: Determine if there are any missing values in the dataset for metrics like IRR, Net IRR, etc.\n   * Strategy Decision: \nSo basically since we want to have a tool that extracts multiple things from the document we should essentially have some time as a summarizer or query engine that looks for each goal and returns the results. \n* Level 2: \"The Imputer\"\n   * Goal: Handle missing values in your sample dataset.\n* Level 3: \"The Tokenizer\"\n   * Goal: Tokenize your sample text data.\n\n\n   * Choose a vector DB - below will depend on what DB you choose and what algo you want to use\n      * Tokenization Method: Choose a method for tokenizing text into individual words or phrases. You can use basic methods like Python's .split() method or more advanced methods like the Natural Language Toolkit (NLTK).\n      * N-grams: Optionally, consider using n-grams to capture context in your tokens.\n      *    * Vectorization: At this point, you might want to convert your tokens into numerical vectors using techniques like TF-IDF (Term Frequency-Inverse Document Frequency) or Word2Vec. Upload to Vector Db of your choosing\n* Level 4: \"Vector Wizard\"\n   * Goal: Convert text into numerical vectors using your chosen technique.\n* Checkpoint: Push current project state to GitHub.\n\ud83c\udf0d World 3: Ground Truth Mountain\n\u23f3 Timeframe: 2 Days\n* Level 1: \"Labeler of Truth\"\n   * Goal: Manually label a small subset of your sample data.\n   * Subset Selection: Choose a manageable subset of your data. The idea is to pick a small but diverse set of samples that represent various scenarios.\n   * Labeling Tool: Use a simple tool (Excel, a Python script with a UI, or even paper) to label the data.\n   * Metrics: Label the metrics that are relevant to your model like IRR, Net IRR, or particular investment strategies.\n   * Annotation Guidelines: Create a small guideline for how to label the data. This will be useful for you and anyone else who might contribute to the labeling process.\n   * Quality Check: Review the labels to ensure consistency and accuracy.\n   * Save Labels: Store these labels in a structured format like CSV or directly in your database.\n\n\n\n\n* Level 2: \"The Splitter\"\n   * Goal: Split your dataset into training, validation, and test sets.\n   * Partitioning: Use commonly accepted ratios for splitting the data. A common split ratio might be 70% for training, 15% for validation, and 15% for testing.\n   * Stratified Sampling: If your data classes are imbalanced, consider using stratified sampling to maintain the class distribution across all sets.\n   * Random Splitting: Make sure to randomize the data before splitting. Many machine learning libraries offer this functionality.\n   * Data Integrity: Ensure that each subset does not leak information to another (i.e., a single data point should belong to only one set: either training, validation, or testing).\n   * Save Sets: Store these subsets in a way that they can be easily loaded for model training and evaluation, perhaps in a database table or as separate CSV files.\n\n\n* Checkpoint: Push current project state to GitHub.\n\ud83c\udf0d World 4: The Model Experimentation Arena\n\u23f3 Timeframe: 2 Weeks\n* Level 1: \"Novice Modeler\"\n   * Goal: Implement simpler models for initial testing.\n* Level 2: \"Pythonic Sage\"\n   * Goal: Have your Python environment fully set up.\n* Level 3: \"Tuner of Parameters\"\n   * Goal: Run hyperparameter tuning on your initial models.\n* Level 4: \"Validator Supreme\"\n   * Goal: Implement k-Fold cross-validation.\n* Level 5: \"Metric Master\"\n   * Goal: Evaluate your model using chosen metrics.\n* Level 6: \"Advanced Model Maestro\"\n   * Goal: Experiment with more advanced models, if needed.\n* Final Boss: \"The Decider\"\n   * Goal: Select the best-performing model based on your evaluation metrics.\n* Checkpoint: Push current project state to GitHub. Celebrate the completion of your MVP!\n   *", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "4"}, "61047cdd-05ad-4973-8020-d90975ce4138": {"__data__": {"id_": "61047cdd-05ad-4973-8020-d90975ce4138", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "hash": "0f192554c68e4c28b92964612fec7a7d9923749487d5882baf509a45c728f8d0", "text": "\ufeffYou can group indexes into different data structures. Notes taken from here\n\n\nVisual representation of each structure \n  \n\n\n\nHash Based - \nThis transforms higher dimensional data into lower dimensional data using hashing. Burring indexing the dataset is hashed multiple times to ensure that similar points are more likely to collide. This is the opposite of what usually happens with hashing algorithms (They try to avoid collisions.)\n\n\nThey are very fast while scaling to huge amounts of data but the downside is they are not very accurate \n\n\nTree Based - \n        Tree based index structures allow for rapid searches in high-dimensional spaces. The tree is constructed so similar data points end up in the subtrees. Spotify developed ANNOY (Approximate Nearest Neighbours Oh Yeah) uses a forrest of binary search trees. \n\n\nThe downside for tree bases indexes is they perform well only on low dimensional data. They can not capture high dimensional data with  accuracy \n\n\nGraph Based - \n        Graph based indexes are based on data points in vector spaces from a graph, where the nodes represent the data values and the edges connecting the nodes represent the similarity between the data points. The graph is constructed in a way the similar data points are more likely to be connected by edges and the ANN search algo is designed to traverse the graph in an efficient manner.\n\n\nInverted File - \n\n\nInverted file index divides the vector space into a number of tessellated cells, Called Voronio diagrams - These reduce the search space similar to clustering. In order to find the NN it must locate the centroid of the nearest Voronoi Cell.\n\n\n\n\nCompression Steps - To improve search accuracy in the retrieval a process called quantization is utilized. Where the underlying vectors in the index are broken into chunks made up of fewer bytes.\n\n\n\n\nCommon Types of indexes - \n\n\nPQ - Performs both compression and data reduction. The idea is to decompose a larger dimensional vector space into a cartesian product of smaller dimensional subspaces by quantizing each subspace into its own cluster\n\n\nFlat - Stores vectors in unmodified form. Used for exact kNN search on every vector. The most accurate but the slowest. This doesn't use compression\n\n\nIVF-Flat - Inverted file indexes rapidly narrow down the search space. This is much faster than brute force search, but they sacrifice some accuracy in the form of recall. \n\n\nIVF-PQ - Uses IVF combo with Product Quantization to compress the vectors. Reducing the memory footprint and speeding up the search, whole being better in recall than a pure PQ index.\n\n\nHNSW - The most popular index, and is often combined with PQ, in the form of HNSW-PQ to improve search speed and memory compared to IVF-PQ.\n\n\nVamana - New index, design for on disk performance storing larger than memory vector data while performing as fast as HNSW - Still in early stages many DB havent implemented this yet.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "4"}, "5963df0f-131f-443c-a668-f4ae1beef373": {"__data__": {"id_": "5963df0f-131f-443c-a668-f4ae1beef373", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "hash": "25b50d60d3f4cb2bea9c8b6741413b34ea19ce15c133c3657b7a6da759ff6df9", "text": "\ufeffBlue means open \n\n\nUTG:\n\n\n  \n\n\n\nMP:\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCO 22%:\n  \n33\n\n\nBTN 35%:", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "4"}, "80fa4dcf-c889-4ec9-85e5-8cd25664f798": {"__data__": {"id_": "80fa4dcf-c889-4ec9-85e5-8cd25664f798", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "hash": "a1afcbcf69fa42458306df63c8dfe3443ec6bfbc827023766761644ce4ee3a16", "text": "\ufeffGoals for the LLM Tool\n\n\n1. IRR, Net IRR, and MIOC: The tool aims to accurately extract and present key financial metrics like Internal Rate of Return (IRR), Net IRR after fees, and Multiple on Invested Capital (MIOC) from investor quarterly memos.\n2. Capital Distribution vs. Capital Calls: For Limited Partners (LPs) who are already invested, the tool will provide information about the amount of capital being distributed versus the capital being called. Special scenarios like recallable distributions will also be considered.\n3. Commitment Pacing: The tool will aim to gauge how quickly the called capital is being invested. Advanced algorithms like Monte Carlo simulations may be used to understand commitment pacing more accurately.\n4. Investment Strategy & 12-Month Roadmap: Another goal is to highlight the key strategies that the fund plans to employ in the next 12 months. This includes the types of investments they're targeting and the expected returns on those investments.\n\nThoughts on these goals? Are these all possible with just the memos or will we need to extract data from other documents? \n\n\nQuestions\n\nA. When it comes to public forms, What specific data do you recommend we look for in 13-f filings?\n\n\nB. You mentioned sometimes companies will provide unaudited Financial statements for companies they have invested in.  \n\n\nDo you see value in incorporating data from unaudited financial statements of the companies the fund has invested in? If so, what specific metrics or data points would be most beneficial?\n\nC. If we get access to a funds data room, Would we want to do data analysis on it? What key documents or data points should our tool aim to extract from the investor data room?\n\nD. Could you provide insights on the important indicators for assessing the timing of a fund's closing? How does this relate to the fund's operational efficiency and fee structure? \n\n\n\n\nCreating and Understanding the tools Pipeline\n\n\nSteps for the tool:\n\n\n1. User uploads a document from a fund. \n2. The agent scans the document Creates a condensed summary for the important financial data. This will include things like IRR, Net IRR, MOC, MIOC, Successful trades made by the fund and Large losses made by the fund. It can export this data into a CSV file. \n3. If you are a LP and the data is in the memo it will then summarize your Capital Distribution vs. Capital Calls: For Limited Partners (LPs) who are already invested, the tool will provide information about the amount of capital being distributed versus the capital being called. Special scenarios like recallable distributions will also be considered.\n4. Commitment Pacing: Not sure how to do this. Will need to circle back to this one\n5. Funds 12 month road map. The tool will highlight the key strategies that the fund plans to employ in the next 12 months. This includes the types of investments they're targeting and the expected returns on those investments.\nPseudo code for tool:\n1. Load the data from the document. \n2. Pre process the text in any way needed\n3. Create nodes with metadata from the text\n   1. This involves choosing a chunk size and creating embeddings\n4. Have an agent look for all mentions of financial data in regards to our search. \n   1. Create a summary of the financial data. Export to a csv file.\n5. Have an Agent scan for any mention of capital distribution of the fund\n   1. Create a summary of the capital distribution vs capital calls. \n6. Have an agent Scan the document for a 12 month road map of upcoming investments \n   1. Create a summary of the capital distribution of upcoming investments and expected return on those investments.\n\n\nGit hubs to look at\n1. Summary.wft github Chunking best practices\n2. Train and fine tune models easily", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "4"}, "556924fb-19c9-4114-a4aa-1b43a395986f": {"__data__": {"id_": "556924fb-19c9-4114-a4aa-1b43a395986f", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "hash": "fbd69a1e34ee05ca4954638a91ba43583cb3b8af888def6ed7726deee7775a6b", "text": "\ufeff* Use LangChain's pipelines like the Question Answering Pipeline to streamline this iterative process. It can retrieve documents, ask questions, evaluate responses, and generate new questions in a loop.\n\n\n* Use a model like Anthropic's Claude or LangChain's QuestionGenerator to dynamically generate questions tailored to each document. These can take into account the existing Q&A pairs and generate follow up questions.\n\n\n* Use a summarization model like Anthropic's Constitutional AI or LangChain's Summarize to get summaries of documents with poor existing Q&A pairs. Generate new questions against these summaries.\n\n\n* Employ an iterative workflow that evaluates existing Q&A pairs to determine quality, then generates new questions on documents/sections with low quality responses.\n\n\n* For batch processing, split the corpus into subsets and run the iterative QA workflow on each subset. Merge the Q&A pairs from each batch.\n\n\n* Use LangChain's LLM utilities like LLM Scorer to automatically evaluate the quality of existing Q&A pairs and determine when new questions are needed.\n\n\n* Store evidence passages for each Q&A pair to enable manual review/validation as needed.\n\n\nRanking in order of difficulty \n1. Use a summarization model like Constitutional AI to summarize documents\n2. Store evidence passages for each Q&A pair\n3. Use a question generator like Claude to dynamically generate questions\n4. Use a passage retriever like Stella to identify relevant sections\n5. Write simple information extraction scripts for structured data\n6. Use an iterative workflow with pipelines to evaluate and generate questions\n\n\nMy current thoughts on the project. Basically we have hit a wall where we have the ability to loop questions over a corpus of documents. The problem we are running into is the bottleneck is the questions we ask the documents. Sometimes they have context but don't return answers because of how the question is structured. \n\n\nWhat I would like to do is find a way to improve this Q&A review process using LLM tools. Since this is a basic function of LLMs. (Reading over text and running sentiment analysis ). \n\nSo looking at the Langchain docs / tutorials I have found an agent that can do exactly this (query an SQL db ) along with doing retrieval over documents. \n\n\nAlso would like to get some more experience setting up some type of Eval system where we can score the answers and outputs vs a baseline to really get a sense of what models and architecture  is the best for this product.  \n\n\nAlso creating just a simple prompt template to summarize each document or get the bullet points on what areas are important could be a useful task. This would be creating a prompt template such as the one this guy does in this video. \n\n\n\n\nSo to summarize here is the order of things I would like to try working on next. \n\n\n1. Debug the current project and find out why the chat bot isn't returning any of our data. \n   1. Looks like normally embedding the data and upserting to pinecone works fine. Potentially the issue is due to how the vectors are set up. It was due to the way the vectors were set up. The vectors needed the metadata to contain a text and source field \n2. Do research on nltpk entity extraction and what is actually happening in the magic box of embeddings and what not. Currently we are using this as the base of our project. \n   1. Looks like this uses Llama index. Basically a langchain type framework  that may have better tools for the question part of our project.\n3. Look for simple ways to find some sort of eval baseline. As outlined in the beginning section of this document\n4. Try this out - Llama index stuff\n5. Try out summaries of the documents like this guys video vs baseline\n6. Create an agent that does this process using these docs doing retrieval over documents. \n\n\n\n\n\n\n\n\nNew to do list. \n\n\n\n\n1.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "4"}, "e5df0c10-431c-4640-8b65-618502387ae6": {"__data__": {"id_": "e5df0c10-431c-4640-8b65-618502387ae6", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {}, "hash": "caae7ed517f0c1392ad2122799aee793d3c2984d3fc21e6d72adfdf2d60f9135", "text": "\ufeffSport is generally recognised as activities based on physical athleticism or physical dexterity.[3] Sports are usually governed by rules to ensure fair competition and consistent adjudication of the winner.\n\n\n\"Sport\" comes from the Old French desport meaning \"leisure\", with the oldest definition in English from around 1300 being \"anything humans find amusing or entertaining\".[4]\n\n\nOther bodies advocate widening the definition of sport to include all physical activity and exercise. For instance, the Council of Europe includes all forms of physical exercise, including those completed just for fun.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "4"}}, "docstore/metadata": {"c5cd10c2-a0bc-4cbe-962c-35abc7913f99": {"doc_hash": "3b1f0b68e3e782105a04f9626a93f596202212440a1ca99b4774b64107795526"}, "5258a827-5bbe-4090-9024-531ca899be59": {"doc_hash": "12b28aa787b31e974d6a9cd38926e7a810a3b33dd4f26bf60477d4078fd17c07"}, "61047cdd-05ad-4973-8020-d90975ce4138": {"doc_hash": "0f192554c68e4c28b92964612fec7a7d9923749487d5882baf509a45c728f8d0"}, "5963df0f-131f-443c-a668-f4ae1beef373": {"doc_hash": "25b50d60d3f4cb2bea9c8b6741413b34ea19ce15c133c3657b7a6da759ff6df9"}, "80fa4dcf-c889-4ec9-85e5-8cd25664f798": {"doc_hash": "a1afcbcf69fa42458306df63c8dfe3443ec6bfbc827023766761644ce4ee3a16"}, "556924fb-19c9-4114-a4aa-1b43a395986f": {"doc_hash": "fbd69a1e34ee05ca4954638a91ba43583cb3b8af888def6ed7726deee7775a6b"}, "e5df0c10-431c-4640-8b65-618502387ae6": {"doc_hash": "caae7ed517f0c1392ad2122799aee793d3c2984d3fc21e6d72adfdf2d60f9135"}}}